{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7df9eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (0.6.0)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (0.6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (3.10.1)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (4.25.7)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from matplotlib->mediapipe) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from matplotlib->mediapipe) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kaito\\documents\\asl-cv-service\\venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# dependencies \n",
    "%pip install mediapipe opencv-python scikit-learn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f7ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cdc342",
   "metadata": {},
   "source": [
    "Setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5624e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
    "data_path1 = 'Public/asl_alphabet_train'\n",
    "data_path = 'Public/train'\n",
    "classifications = os.listdir(data_path)\n",
    "classifications.sort()\n",
    "print(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b428ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[landmark {\n",
      "  x: 0.457428038\n",
      "  y: 0.583321\n",
      "  z: -6.47400668e-007\n",
      "}\n",
      "landmark {\n",
      "  x: 0.570786357\n",
      "  y: 0.503428459\n",
      "  z: -0.0355797932\n",
      "}\n",
      "landmark {\n",
      "  x: 0.640131116\n",
      "  y: 0.373975933\n",
      "  z: -0.0444065407\n",
      "}\n",
      "landmark {\n",
      "  x: 0.651733875\n",
      "  y: 0.264574826\n",
      "  z: -0.0550685\n",
      "}\n",
      "landmark {\n",
      "  x: 0.634490609\n",
      "  y: 0.179893538\n",
      "  z: -0.0590976737\n",
      "}\n",
      "landmark {\n",
      "  x: 0.545696855\n",
      "  y: 0.270556241\n",
      "  z: -0.00423305947\n",
      "}\n",
      "landmark {\n",
      "  x: 0.586574137\n",
      "  y: 0.209667951\n",
      "  z: -0.0716262534\n",
      "}\n",
      "landmark {\n",
      "  x: 0.59239167\n",
      "  y: 0.307403505\n",
      "  z: -0.111358762\n",
      "}\n",
      "landmark {\n",
      "  x: 0.589894652\n",
      "  y: 0.393817753\n",
      "  z: -0.122083895\n",
      "}\n",
      "landmark {\n",
      "  x: 0.467299551\n",
      "  y: 0.282378018\n",
      "  z: -0.00464758603\n",
      "}\n",
      "landmark {\n",
      "  x: 0.517436743\n",
      "  y: 0.234142944\n",
      "  z: -0.077506274\n",
      "}\n",
      "landmark {\n",
      "  x: 0.530205071\n",
      "  y: 0.349560529\n",
      "  z: -0.0962087959\n",
      "}\n",
      "landmark {\n",
      "  x: 0.534969032\n",
      "  y: 0.43843478\n",
      "  z: -0.0864398256\n",
      "}\n",
      "landmark {\n",
      "  x: 0.39551264\n",
      "  y: 0.304352134\n",
      "  z: -0.0160673223\n",
      "}\n",
      "landmark {\n",
      "  x: 0.45714274\n",
      "  y: 0.292900413\n",
      "  z: -0.0848888382\n",
      "}\n",
      "landmark {\n",
      "  x: 0.469179869\n",
      "  y: 0.400668502\n",
      "  z: -0.0749421343\n",
      "}\n",
      "landmark {\n",
      "  x: 0.468660116\n",
      "  y: 0.472125232\n",
      "  z: -0.0443902127\n",
      "}\n",
      "landmark {\n",
      "  x: 0.325684965\n",
      "  y: 0.335119277\n",
      "  z: -0.0306416415\n",
      "}\n",
      "landmark {\n",
      "  x: 0.391185641\n",
      "  y: 0.321072429\n",
      "  z: -0.0701518431\n",
      "}\n",
      "landmark {\n",
      "  x: 0.40494737\n",
      "  y: 0.404261172\n",
      "  z: -0.0554431491\n",
      "}\n",
      "landmark {\n",
      "  x: 0.403822094\n",
      "  y: 0.457565784\n",
      "  z: -0.0288787335\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(r\"Public/asl_alphabet_train/A/A1.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "print(hands.process(img).multi_hand_landmarks)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e6057",
   "metadata": {},
   "source": [
    "Extracting media pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5869b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:50<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for label in tqdm(classifications):\n",
    "    folder = os.path.join(data_path, label)\n",
    "    count = 0\n",
    "    for file in os.listdir(folder):\n",
    "        if label == 'del' or label == 'nothing' or label == 'space':\n",
    "            break\n",
    "        if count > 50:\n",
    "            break\n",
    "        count += 1\n",
    "        if file.endswith('.jpg'):\n",
    "            img_path = os.path.join(folder, file)\n",
    "            img= cv2.imread(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(img)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                hand = results.multi_hand_landmarks[0]\n",
    "                landmark_list = []\n",
    "                for landmark in hand.landmark:\n",
    "                    landmark_list.extend([landmark.x, landmark.y, landmark.z])\n",
    "                if len(landmark_list) == 63:\n",
    "                    x.append(landmark_list)\n",
    "                    y.append(label.lower())\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f671e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1508/1508 [01:10<00:00, 21.45it/s]\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "test_path = 'Public/train'\n",
    "test_files = sorted(os.listdir(test_path))  # this is the actual list of filenames\n",
    "\n",
    "for file in tqdm(test_files):\n",
    "    if file.endswith('.jpg'):\n",
    "        img_path = os.path.join(test_path, file)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand = results.multi_hand_landmarks[0]\n",
    "            landmark_list = []\n",
    "            for landmark in hand.landmark:\n",
    "                landmark_list.extend([landmark.x, landmark.y, landmark.z])\n",
    "            if len(landmark_list) == 63:\n",
    "                x.append(landmark_list)\n",
    "                y.append(file[0])  # label like 'A', 'SPACE'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "abaaf32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to processed_landmarks.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(x)\n",
    "df['label'] = y\n",
    "df.to_csv('processed_landmarks_combined_lite.csv', index=False)\n",
    "print(\"Saved to processed_landmarks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a24a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 30.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# x_test = []\n",
    "# y_test = []\n",
    "\n",
    "# test_path = 'Public/asl_alphabet_test'\n",
    "# test_files = sorted(os.listdir(test_path))  # this is the actual list of filenames\n",
    "\n",
    "# for file in tqdm(test_files):\n",
    "#     if file.endswith('.jpg'):\n",
    "#         img_path = os.path.join(test_path, file)\n",
    "#         img = cv2.imread(img_path)\n",
    "\n",
    "#         if img is None:\n",
    "#             continue\n",
    "\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         results = hands.process(img)\n",
    "\n",
    "#         if results.multi_hand_landmarks:\n",
    "#             hand = results.multi_hand_landmarks[0]\n",
    "#             landmark_list = []\n",
    "#             for landmark in hand.landmark:\n",
    "#                 landmark_list.extend([landmark.x, landmark.y, landmark.z])\n",
    "#             if len(landmark_list) == 63:\n",
    "#                 x_test.append(landmark_list)\n",
    "#                 y_test.append(file.split('_')[0])  # label like 'A', 'SPACE'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ff610067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:02<00:00, 24.01it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "test_path = 'Public/test'\n",
    "test_files = sorted(os.listdir(test_path))  # this is the actual list of filenames\n",
    "\n",
    "for file in tqdm(test_files):\n",
    "    if file.endswith('.jpg'):\n",
    "        img_path = os.path.join(test_path, file)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand = results.multi_hand_landmarks[0]\n",
    "            landmark_list = []\n",
    "            for landmark in hand.landmark:\n",
    "                landmark_list.extend([landmark.x, landmark.y, landmark.z])\n",
    "            if len(landmark_list) == 63:\n",
    "                x_test.append(landmark_list)\n",
    "                y_test.append(file[0])  # label like 'A', 'SPACE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96333dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 27/28 [00:01<00:00, 21.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# test_path = 'Public/asl_alphabet_test'\n",
    "# test_files = sorted(os.listdir(test_path))  # this is the actual list of filenames\n",
    "\n",
    "# for file in tqdm(test_files):\n",
    "#     if file.endswith('.jpg'):\n",
    "#         img_path = os.path.join(test_path, file)\n",
    "#         img = cv2.imread(img_path)\n",
    "\n",
    "#         if img is None:\n",
    "#             continue\n",
    "\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         results = hands.process(img)\n",
    "\n",
    "#         if results.multi_hand_landmarks:\n",
    "#             hand = results.multi_hand_landmarks[0]\n",
    "#             landmark_list = []\n",
    "#             for landmark in hand.landmark:\n",
    "#                 landmark_list.extend([landmark.x, landmark.y, landmark.z])\n",
    "#             if len(landmark_list) == 63:\n",
    "#                 name = file.split('_')[0]\n",
    "#                 if name== \"nothing\" or name == \"space\" or name == \"del\":\n",
    "#                     break\n",
    "\n",
    "#                 x_test.append(landmark_list)\n",
    "#                 y_test.append(name.lower)  # label like 'A', 'SPACE'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8e1a2600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 61\n",
      "[ 0  1  1  1  2  2  2  3  5  5  6  6  6  6  7  7  8  8  9  9  9  9 10 10\n",
      " 10 12 12 13 14 14 14 15 16 16 17 17 18 18 18 19 19 19 19 19 20 20 21 21\n",
      " 21 21 22 23 23 23 23 24 24 25 25 25 25]\n",
      "[26 26 26 ... 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test), len(y_test))\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x)   \n",
    "x_test = scaler.transform(x_test)\n",
    "# x_train = x\n",
    "# x_test = x_test\n",
    "print(y_test)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "acf78691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#train\n",
    "clf = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=1000, random_state=42)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "#save model\n",
    "joblib.dump(clf, 'asl_model.pkl')\n",
    "#save scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c79858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaito\\Documents\\asl-cv-service\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=42)\n",
    "#train\n",
    "clf = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n",
    "clf.fit(x_train, y_train)\n",
    "#save model\n",
    "joblib.dump(clf, 'asl_model.pkl')\n",
    "#save scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0498dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  2  2  2  3  5  5  6  6  6  6  7  7  8  8  9  9  9  9 10 10\n",
      " 10 12 12 13 14 14 14 15 16 16 17 17 18 18 18 19 19 19 19 19 20 20 21 21\n",
      " 21 21 22 23 23 23 23 24 24 25 25 25 25]\n",
      "[ 0 48  1  1  2  1  2  3  5  5  6  6  6  6  7  7  8  4  9  9  9  9 20 10\n",
      " 36 13 12  4 14 41  4 15 16  9 17  2 19 18 18 18 19 19 13 19 20 20 17 36\n",
      " 21 21 22 25 23 23 23 24 24 25 25 25 25]\n",
      "Accuracy: 72.13%\n"
     ]
    }
   ],
   "source": [
    "acc = clf.score(x_test, y_test)\n",
    "predictions = clf.predict(x_test)\n",
    "print(y_test)\n",
    "print(predictions)\n",
    "print(f\"Accuracy: {acc:.2%}\")\n",
    "with open('class_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(classifications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a28f2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 406, 'Z': 404, 'Y': 404, 'K': 403, 'P': 402, 'S': 395, 'D': 392, 'R': 392, 'C': 390, 'F': 387, 'H': 382, 'Q': 380, 'G': 379, 'I': 374, 'E': 368, 'U': 367, 'T': 366, 'A': 358, 'B': 355, 'X': 352, 'J': 352, 'V': 351, 'L': 351, 'W': 333, 'space': 311, 'del': 311, 'M': 222, 'N': 134})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
